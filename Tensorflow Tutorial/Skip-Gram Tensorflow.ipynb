{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1b5e79d89fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutilis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utilis'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Using cached utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e1b4f4a21deb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of training examples:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of validation examples:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "# word2vec - Skip-gram\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\"\"\"\n",
    "ex. input: the first cs224n homework was alot of fun\n",
    "\n",
    "window size: 1 ex. ([left_word,right_word], center_word), (... \n",
    "\n",
    "SkipGram - tries to predict each context word from its target word,\n",
    "\"\"\"\n",
    "\n",
    "#define constants\n",
    "batch_size = 128\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128 #Dim of embedding vector\n",
    "num_sampled = 64 #Number of negative examples to sample\n",
    "\n",
    "\n",
    "train_data, val_data, reverse_dictionary = load_data()\n",
    "print(\"Number of training examples:\", len(train_data)*batch_size)\n",
    "print(\"Number of validation examples:\",len(val_data))\n",
    "\n",
    "def skipgram():\n",
    "    batch_inputs = tf.placeholder(tf.int32,shape=[batch_size,])\n",
    "    batch_labels = tf.placeholder(tf.int32, shape=[batch_size,1]) #row vecto\n",
    "    val_dataset = tf.constant(val_data, dtype=tf.int32)\n",
    "\n",
    "    with tf.variable_scope(\"word2vec\") as scope:\n",
    "        embeddings = tf.variable(t.random.uniform([voabulary_size,\n",
    "                                                  embedding_size],\n",
    "                                                 -1.0,1.0))\n",
    "        batch_embeddings = tf.nn.embedding_lookup(embeddings, batch_size)\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal([vocabulary_size,\n",
    "                                                  embedding_size],\n",
    "                                                 stddev = 1.0/math.sqrt(embedding_size)))\n",
    "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights =wiegths,\n",
    "                                     biases =biases,\n",
    "                                     labels = batch_labels,\n",
    "                                     inputs = batch_inputs,\n",
    "                                     num_sampled = num_sampled,\n",
    "                                     num_classes = vocabulary_size))\n",
    "\n",
    "norm = tf.sqrt(tf.reduce_mean(tf.square(embeddings),1,keepdims=True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "\n",
    "val_embeddings = tf.nn.emmbedding_lookup(normalized_embeddings, val_dataset)\n",
    "similarity = tf.matmul(val_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "return batch_inputs, batch_labels, normalized_embeddings, loss, similarity\n",
    "\n",
    "def run():\n",
    "    batch_input, batch_labels, normalized_embeddings, loss, similarity = skipgram()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        average_loss = 0.0\n",
    "        for step,batch_data in enumerate(train_data):\n",
    "            inputs, labels = batch_data\n",
    "            feed_dict = {batch_inputs: inputs, batch_labels: labels}\n",
    "            \n",
    "            _, loss_val = session.run([optimizer,loss],feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 1000 == 0 :\n",
    "                if step > 0:\n",
    "                    average_loss /= 1000\n",
    "                print('loss at iter', step,\":\", average_loss)\n",
    "                avergae_loss = 0\n",
    "                \n",
    "            if step % 5000 == 0:\n",
    "                #checkin word similarity\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(len(val_data)):\n",
    "                    top_k = 8\n",
    "                    nearest = (-sim[i:]).argsort()[1:top_k+1]\n",
    "                    print_closest_word(val_data[i], nearest, reverse_dictionary)\n",
    "        final_embeddings = normalized_embedding.eval()\n",
    "\n",
    "#start training\n",
    "final_embeddings = run()\n",
    "\n",
    "#Visualize the embedding\n",
    "visuallize_embeddingd(final_embeddings, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
